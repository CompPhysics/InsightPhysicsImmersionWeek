TITLE: INSIGHT Physics Immersion Week (June 21 - June 25), Eigenvalue problems
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: June 23, 2021



!split
===== Eigenvalue problems, basic definitions =====
!bblock 
Let us consider the matrix $\mathbf{A}$ of dimension $n$. The eigenvalues of
$\mathbf{A}$ are defined through the matrix equation 
!bt
\[
   \mathbf{A}\mathbf{x}^{(\nu)} = \lambda^{(\nu)}\mathbf{x}^{(\nu)},
\]
!et
where $\lambda^{(\nu)}$ are the eigenvalues and $\mathbf{x}^{(\nu)}$ the
corresponding eigenvectors.
Unless otherwise stated, when we use the wording eigenvector we mean the
right eigenvector. The left eigenvalue problem is defined as 
!bt
\[
\mathbf{x}^{(\nu)}_L\mathbf{A} = \lambda^{(\nu)}\mathbf{x}^{(\nu)}_L
\]
!et
The above right eigenvector problem is equivalent to a set of $n$ equations with $n$ unknowns
$x_i$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
The eigenvalue problem can be rewritten as 
!bt
\[
   \left( \mathbf{A}-\lambda^{(\nu)} \mathbf{I} \right) \mathbf{x}^{(\nu)} = 0,
\]
!et
with $\mathbf{I}$ being the unity matrix. This equation provides
a solution to the problem if and only if the determinant
is zero, namely
!bt
\[
   \left| \mathbf{A}-\lambda^{(\nu)}\mathbf{I}\right| = 0,
\]
!et
which in turn means that the determinant is a polynomial
of degree $n$ in $\lambda$ and in general we will have 
$n$ distinct zeros. 
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
The eigenvalues of a matrix 
$\mathbf{A}\in {\mathbb{C}}^{n\times n}$
are thus the $n$ roots of its characteristic polynomial 
!bt
\[
P(\lambda) = det(\lambda\mathbf{I}-\mathbf{A}),
\]
!et
or 
!bt
\[
  P(\lambda)= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right).
\]
!et
The set of these roots is called the spectrum and is denoted as
$\lambda(\mathbf{A})$.
If $\lambda(\mathbf{A})=\left\{\lambda_1,\lambda_2,\dots ,\lambda_n\right\}$ then we have
!bt
\[
   det(\mathbf{A})= \lambda_1\lambda_2\dots\lambda_n, 
\]
!et
and if we define the trace of $\mathbf{A}$ as
!bt
\[
Tr(\mathbf{A})=\sum_{i=1}^n a_{ii}\]
!et
then
!bt
\[
Tr(\mathbf{A})=\lambda_1+\lambda_2+\dots+\lambda_n.
\]
!et
!eblock


!split
===== Abel-Ruffini Impossibility Theorem =====
!bblock 
The *Abel-Ruffini* theorem (also known as Abel's impossibility theorem) 
states that there is no general solution in radicals to polynomial equations of degree five or higher.

The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. 
In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.

The theorem only concerns the form that such a solution must take. The content of the theorem is 
that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation $ax^n = b$, are always solvable with a radical.
!eblock


!split
===== Abel-Ruffini Impossibility Theorem =====
!bblock 

The *Abel-Ruffini* theorem says that there are some fifth-degree equations whose solution cannot be so expressed. 
The equation $x^5 - x + 1 = 0$ is an example. Some other fifth degree equations can be solved by radicals, 
for example $x^5 - x^4 - x + 1 = 0$. The precise criterion that distinguishes between those equations that can be solved 
by radicals and those that cannot was given by Galois and is now part of Galois theory: 
a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.

Today, in the modern algebraic context, we say that second, third and fourth degree polynomial 
equations can always be solved by radicals because the symmetric groups $S_2, S_3$ and $S_4$ are solvable groups, 
whereas $S_n$ is not solvable for $n \ge 5$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
In the present discussion we assume that our matrix is real and symmetric, that is 
$\mathbf{A}\in {\mathbb{R}}^{n\times n}$.
The matrix $\mathbf{A}$ has $n$ eigenvalues
$\lambda_1\dots \lambda_n$ (distinct or not). Let $\mathbf{D}$ be the
diagonal matrix with the eigenvalues on the diagonal
!bt
\[
\mathbf{D}=    \left( \begin{array}{ccccccc} \lambda_1 & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & \lambda_2 & 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & \lambda_3 & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\lambda_{n-1} & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & \lambda_n

             \end{array} \right).
\]
!et
If $\mathbf{A}$ is real and symmetric then there exists a real orthogonal matrix $\mathbf{S}$ such that
!bt
\[
     \mathbf{S}^T \mathbf{A}\mathbf{S}= \mathrm{diag}(\lambda_1,\lambda_2,\dots ,\lambda_n),
\]
!et
and for $j=1:n$ we have $\mathbf{A}\mathbf{S}(:,j) = \lambda_j \mathbf{S}(:,j)$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
To obtain the eigenvalues of $\mathbf{A}\in {\mathbb{R}}^{n\times n}$,
the strategy is to
perform a series of similarity transformations on the original
matrix $\mathbf{A}$, in order to reduce it either into a  diagonal form as above
or into a  tridiagonal form. 

We say that a matrix $\mathbf{B}$ is a similarity
transform  of  $\mathbf{A}$ if 
!bt
\[
     \mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S}, \hspace{1cm} \mathrm{where} \hspace{1cm}  \mathbf{S}^T\mathbf{S}=\mathbf{S}^{-1}\mathbf{S} =\mathbf{I}.
\]
!et
The importance of a similarity transformation lies in the fact that
the resulting matrix has the same
eigenvalues, but the eigenvectors are in general different. 
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
To prove this we
start with  the eigenvalue problem and a similarity transformed matrix $\mathbf{B}$.
!bt
\[
   \mathbf{A}\mathbf{x}=\lambda\mathbf{x} \hspace{1cm} \mathrm{and}\hspace{1cm} 
    \mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S}.
\]
!et
We multiply the first equation on the left by $\mathbf{S}^T$ and insert
$\mathbf{S}^{T}\mathbf{S} = \mathbf{I}$ between $\mathbf{A}$ and $\mathbf{x}$. Then we get
!bt
\begin{equation}
   (\mathbf{S}^T\mathbf{A}\mathbf{S})(\mathbf{S}^T\mathbf{x})=\lambda\mathbf{S}^T\mathbf{x} ,
\end{equation}  
!et
which is the same as 
!bt
\[
   \mathbf{B} \left ( \mathbf{S}^T\mathbf{x} \right ) = \lambda \left (\mathbf{S}^T\mathbf{x}\right ).
\]
!et
The variable  $\lambda$ is an eigenvalue of $\mathbf{B}$ as well, but with
eigenvector $\mathbf{S}^T\mathbf{x}$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock  
The basic philosophy is to
 * Either apply subsequent similarity transformations (direct method) so that 
!bt
\begin{equation}
   \mathbf{S}_N^T\dots \mathbf{S}_1^T\mathbf{A}\mathbf{S}_1\dots \mathbf{S}_N=\mathbf{D} ,
\end{equation}
!et
 * Or apply subsequent similarity transformations so that $\mathbf{A}$ becomes tridiagonal (Householder) or upper/lower triangular (the *QR* method to be discussed later). 
 * Thereafter, techniques for obtaining eigenvalues from tridiagonal matrices can be used.
 * Or use so-called power methods
 * Or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.
!eblock



!split
===== Discussion of methods for eigenvalues =====
!bblock The general overview

One speaks normally of two main approaches to solving the eigenvalue problem.
 * The first is the formal method, involving determinants and the  characteristic polynomial. This proves how many eigenvalues  there are, and is the way most of you learned about how to solve the eigenvalue problem, but for matrices of dimensions greater than 2 or 3, it is rather impractical.

 * The other general approach is to use similarity or unitary tranformations  to reduce a matrix to diagonal form. This is normally done in two steps: first reduce to for example a *tridiagonal* form, and then to diagonal form. The main algorithms we will discuss in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this methodology. 
!eblock


!split
===== Direct methods  =====
!bblock 
Direct or non-iterative methods  require for matrices of dimensionality $n\times n$ typically $O(n^3)$ operations. These methods are normally called standard methods and are used for dimensionalities
$n \sim 10^5$ or smaller. A brief historical overview  

|----------------------------------------|
| Year  | $n$          |                 |
|----------------------------------------|
|  1950 | $n=20$       |(Wilkinson)      |
|  1965 | $n=200$      |(Forsythe et al.)|
|  1980 | $n=2000$     |Linpack          |
|  1995 | $n=20000$    |Lapack           |
|  Present | $n\sim 10^6$ |Lapack           |
|----------------------------------------|

shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of
$10^4$. However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost $10^{15}$. We see clearly played out in history the $O(n^3)$ bottleneck  of direct matrix algorithms.

Sloppily speaking, when  $n\sim 10^4$ is cubed we have $O(10^{12})$ operations, which is smaller than the $10^{15}$ increase in flops.  

!eblock


!split
===== Beyond Direct methods  =====
!bblock 
If the matrix to diagonalize is large and sparse, direct methods simply become impractical, 
also because
many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the 
$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. 
Given a matrix $\mathbf{A}$ and a vector $\mathbf{v}$, the associated Krylov sequences of vectors
(and thereby subspaces) 
$\mathbf{v}$, $\mathbf{A}\mathbf{v}$, $\mathbf{A}^2\mathbf{v}$, $\mathbf{A}^3\mathbf{v},\dots$, represent
successively larger Krylov subspaces. 

|----------------------------------------------------------------------|
| Matrix |$\mathbf{A}\mathbf{x}=\mathbf{b}$ |$\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$    | 
|--------------l----------------l---------------------l----------------|
|   $\mathbf{A}=\mathbf{A}^*$ | Conjugate gradient    | Lanczos              |
|  $\mathbf{A}\ne \mathbf{A}^*$  |GMRES etc           |Arnoldi               |
|----------------------------------------------------------------------|

!eblock


!split
===== Methods for finding Eigenpairs (eigenvalues and eigenvectors) =====
!bblock 

The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK and LINPACK.
 * LINPACK: package for linear equations  and least square problems.
 * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website URL: "http://www.netlib.org"  it is  possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.
 * BLAS (I, II and III): (Basic Linear Algebra Subprograms)  are routines that provide standard building blocks for performing basic vector and matrix operations.   Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. 

!eblock



!split
===== Developing your own code =====
!bblock 
We can rewrite  our original differential equation in terms of a discretized equation with approximations to the 
derivatives as
!bt
\[
    -\frac{u_{i+1} -2u_i +u_{i-i}}{h^2}=f(x_i,u(x_i)),
\]
!et
with $i=1,2,\dots, n$. We need to add to this system the two boundary conditions $u(a) =u_0$ and $u(b) = u_{n+1}$.
If we define a matrix
!bt 
\[
    \mathbf{A} = \frac{1}{h^2}\left(\begin{array}{cccccc}
                          2 & -1 &  &   &  & \\
                          -1 & 2 & -1 & & & \\
                           & -1 & 2 & -1 & &  \\
                           & \dots   & \dots &\dots   &\dots & \dots \\
                           &   &  &-1  &2& -1 \\
                           &    &  &   &-1 & 2 \\
                      \end{array} \right)
\]
!et
and the corresponding vectors $\mathbf{u} = (u_1, u_2, \dots,u_n)^T$ and 
$\mathbf{f}(\mathbf{u}) = f(x_1,x_2,\dots, x_n,u_1, u_2, \dots,u_n)^T$  we can rewrite the differential equation
including the boundary conditions as a system of linear equations with  a large number of unknowns 
!bt
\[
   \mathbf{A}\mathbf{u} = \mathbf{f}(\mathbf{u}).
 \]
!et
!eblock


!split
===== Discretizing a Continuous Equation =====
!bblock 
We are first interested in the solution of the radial part of Schroedinger's equation for one electron.

Assume we are in one dimension and let $x$ be the variable of interest and u(x), the solution to the differential equation
!bt
\[
  -\frac{\hbar^2}{2 m} \frac{d^2 u}{dx^2}+ V(x) u(x) = E u(x).
\]
!et
If $V(x)$ is the harmonic oscillator potential $(1/2)kr^2$ with
$k=m\omega^2$ and $E$ is
the energy of the harmonic oscillator in one dimensions, with 
the oscillator frequency $\omega$, the energies are
!bt
\[
E_{nl}=  \hbar \omega \left(n+\frac{1}{2}\right),
\]
!et
with $n=0,1,2,\dots$ and $l=0,1,2,\dots$.
!eblock



The boundary conditions for say the one-dimensional harmonic oscilaltor problem are $u(-\infty)=0$ and $u(\infty)=0$.



!split
===== Dimensionless Variable =====
!bblock 
We introduce a dimensionless variable $\rho = (1/\alpha) x$
where $\alpha$ is a constant with dimension length and get
!bt
\[
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       +  V(\rho){\rho^2}u(\rho)  = E u(\rho) .
\]
!et
Inserting the harmonic oscillator potential $V(\rho) = (1/2) k \alpha^2\rho^2$ we end up with
!bt
\[
  -\frac{\hbar^2}{2 m \alpha^2} \frac{d^2}{d\rho^2} u(\rho) 
       + \frac{k}{2} \alpha^2\rho^2u(\rho)  = E u(\rho) .
\]
!et
We multiply thereafter with $2m\alpha^2/\hbar^2$ on both sides and obtain
!bt
\[
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
\]
!et
!eblock


!split
===== Rewriting the Equation =====
!bblock 
We have thus
!bt
\[
  -\frac{d^2}{d\rho^2} u(\rho) 
       + \frac{mk}{\hbar^2} \alpha^4\rho^2u(\rho)  = \frac{2m\alpha^2}{\hbar^2}E u(\rho) .
\]
!et
The constant $\alpha$ can now be fixed
so that
!bt
\[
\frac{mk}{\hbar^2} \alpha^4 = 1,
\]
!et
or 
!bt
\[
\alpha = \left(\frac{\hbar^2}{mk}\right)^{1/4}.
\]
!et
Defining
!bt 
\[
\lambda = \frac{2m\alpha^2}{\hbar^2}E,
\]
!et
we can rewrite Schroedinger's equation as
!bt
\[
  -\frac{d^2}{d\rho^2} u(\rho) + \rho^2u(\rho)  = \lambda u(\rho) .
\]
!et
This is the first equation to solve numerically. In one dimension 
the eigenvalues for $l=0$ are 
$\lambda_0=1,\lambda_1=3,\lambda_2=5,\dots .$
!eblock


!split
===== Boundary values =====
!bblock 
We use the by now standard
expression for the second derivative of a function $u$
!bt
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    label{eq:diffoperation}
\end{equation} 
!et
where $h$ is our step.
Next we define minimum and maximum values for the variable $\rho$,
$\rho_{\mathrm{min}}$  and $\rho_{\mathrm{max}}$, respectively.
You need to check your results for the energies against different values
$\rho_{\mathrm{max}}$, since we cannot set
$\rho_{\mathrm{max}}=\infty$. 
!eblock


!split
===== Discretization Variables =====
!bblock 
With a given number of steps, $n_{\mathrm{step}}$, we then 
define the step $h$ as
!bt
\[
  h=\frac{\rho_{\mathrm{max}}-\rho_{\mathrm{min}} }{n_{\mathrm{step}}}.
\]
!et
Define an arbitrary value of $\rho$ as 
!bt
\[
    \rho_i= \rho_{\mathrm{min}} + ih \hspace{1cm} i=0,1,2,\dots , n_{\mathrm{step}}
\]
!et
we can rewrite the Schr\"odinger equation for $\rho_i$ as
!bt
\[
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}+\rho_i^2u(\rho_i)  = \lambda u(\rho_i),
\]
!et
or in  a more compact way
!bt
\[
-\frac{u_{i+1} -2u_i +u_{i-1}}{h^2}+\rho_i^2u_i=-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}+V_iu_i  = \lambda u_i,
\]
!et
where $V_i=\rho_i^2$ is the harmonic oscillator potential.
!eblock


!split
===== As a eigenvalue problem =====
!bblock 
Define first the diagonal matrix element
!bt
\[
   d_i=\frac{2}{h^2}+V_i,
\]
!et
and the non-diagonal matrix element 
!bt
\[
   e_i=-\frac{1}{h^2}.
\]
!et
In this case the non-diagonal matrix elements are given by a mere constant. *All non-diagonal matrix elements are equal*.

With these definitions the Schroedinger equation takes the following form
!bt
\[
d_iu_i+e_{i-1}u_{i-1}+e_{i+1}u_{i+1}  = \lambda u_i,
\]
!et
where $u_i$ is unknown. We can write the 
latter equation as a matrix eigenvalue problem 
!bt
\begin{equation}
    \left( \begin{array}{ccccccc} d_1 & e_1 & 0   & 0    & \dots  &0     & 0 \\
                                e_1 & d_2 & e_2 & 0    & \dots  &0     &0 \\
                                0   & e_2 & d_3 & e_3  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &d_{n_{\mathrm{step}}-2} & e_{n_{\mathrm{step}}-1}\\
                                0   & \dots & \dots & \dots  &\dots       &e_{n_{\mathrm{step}}-1} & d_{n_{\mathrm{step}}-1}

             \end{array} \right)      \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right)=\lambda \left( \begin{array}{c} u_{1} \\
                                                              u_{2} \\
                                                              \dots\\ \dots\\ \dots\\
                                                              u_{n_{\mathrm{step}}-1}
             \end{array} \right) 
      label{eq:sematrix}
\end{equation} 
!et
or if we wish to be more detailed, we can write the tridiagonal matrix as
!bt
\begin{equation}
    \left( \begin{array}{ccccccc} \frac{2}{h^2}+V_1 & -\frac{1}{h^2} & 0   & 0    & \dots  &0     & 0 \\
                                -\frac{1}{h^2} & \frac{2}{h^2}+V_2 & -\frac{1}{h^2} & 0    & \dots  &0     &0 \\
                                0   & -\frac{1}{h^2} & \frac{2}{h^2}+V_3 & -\frac{1}{h^2}  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\frac{2}{h^2}+V_{n_{\mathrm{step}}-2} & -\frac{1}{h^2}\\
                                0   & \dots & \dots & \dots  &\dots       &-\frac{1}{h^2} & \frac{2}{h^2}+V_{n_{\mathrm{step}}-1}

             \end{array} \right)  
label{eq:matrixse} 
\end{equation} 
!et
Recall that the solutions are known via the boundary conditions at
$i=n_{\mathrm{step}}$ and at the other end point, that is for  $\rho_0$.
The solution is zero in both cases.
!eblock


