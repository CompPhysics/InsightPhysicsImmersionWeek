{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- dom:TITLE: INSIGHT Physics Immersion Week (June 21 - June 25), Eigenvalue problems, Lecture Wednesday June 23 -->\n",
    "# INSIGHT Physics Immersion Week (June 21 - June 25), Eigenvalue problems, Lecture Wednesday June 23\n",
    "<!-- dom:AUTHOR: Morten Hjorth-Jensen at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University -->\n",
    "<!-- Author: -->  \n",
    "**Morten Hjorth-Jensen**, Department of Physics, University of Oslo and Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University\n",
    "\n",
    "Date: **June 23, 2021**\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "Let us consider the matrix $\\mathbf{A}$ of dimension $n$. The eigenvalues of\n",
    "$\\mathbf{A}$ are defined through the matrix equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x}^{(\\nu)} = \\lambda^{(\\nu)}\\mathbf{x}^{(\\nu)},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\lambda^{(\\nu)}$ are the eigenvalues and $\\mathbf{x}^{(\\nu)}$ the\n",
    "corresponding eigenvectors.\n",
    "Unless otherwise stated, when we use the wording eigenvector we mean the\n",
    "right eigenvector. The left eigenvalue problem is defined as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}^{(\\nu)}_L\\mathbf{A} = \\lambda^{(\\nu)}\\mathbf{x}^{(\\nu)}_L\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above right eigenvector problem is equivalent to a set of $n$ equations with $n$ unknowns\n",
    "$x_i$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "The eigenvalue problem can be rewritten as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left( \\mathbf{A}-\\lambda^{(\\nu)} \\mathbf{I} \\right) \\mathbf{x}^{(\\nu)} = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\mathbf{I}$ being the unity matrix. This equation provides\n",
    "a solution to the problem if and only if the determinant\n",
    "is zero, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\left| \\mathbf{A}-\\lambda^{(\\nu)}\\mathbf{I}\\right| = 0,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which in turn means that the determinant is a polynomial\n",
    "of degree $n$ in $\\lambda$ and in general we will have \n",
    "$n$ distinct zeros.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "The eigenvalues of a matrix \n",
    "$\\mathbf{A}\\in {\\mathbb{C}}^{n\\times n}$\n",
    "are thus the $n$ roots of its characteristic polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\lambda) = det(\\lambda\\mathbf{I}-\\mathbf{A}),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(\\lambda)= \\prod_{i=1}^{n}\\left(\\lambda_i-\\lambda\\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The set of these roots is called the spectrum and is denoted as\n",
    "$\\lambda(\\mathbf{A})$.\n",
    "If $\\lambda(\\mathbf{A})=\\left\\{\\lambda_1,\\lambda_2,\\dots ,\\lambda_n\\right\\}$ then we have"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "det(\\mathbf{A})= \\lambda_1\\lambda_2\\dots\\lambda_n,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we define the trace of $\\mathbf{A}$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Tr(\\mathbf{A})=\\sum_{i=1}^n a_{ii}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Tr(\\mathbf{A})=\\lambda_1+\\lambda_2+\\dots+\\lambda_n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abel-Ruffini Impossibility Theorem\n",
    "The *Abel-Ruffini* theorem (also known as Abel's impossibility theorem) \n",
    "states that there is no general solution in radicals to polynomial equations of degree five or higher.\n",
    "\n",
    "The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. \n",
    "In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.\n",
    "\n",
    "The theorem only concerns the form that such a solution must take. The content of the theorem is \n",
    "that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation $ax^n = b$, are always solvable with a radical.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Abel-Ruffini Impossibility Theorem\n",
    "\n",
    "The *Abel-Ruffini* theorem says that there are some fifth-degree equations whose solution cannot be so expressed. \n",
    "The equation $x^5 - x + 1 = 0$ is an example. Some other fifth degree equations can be solved by radicals, \n",
    "for example $x^5 - x^4 - x + 1 = 0$. The precise criterion that distinguishes between those equations that can be solved \n",
    "by radicals and those that cannot was given by Galois and is now part of Galois theory: \n",
    "a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.\n",
    "\n",
    "Today, in the modern algebraic context, we say that second, third and fourth degree polynomial \n",
    "equations can always be solved by radicals because the symmetric groups $S_2, S_3$ and $S_4$ are solvable groups, \n",
    "whereas $S_n$ is not solvable for $n \\ge 5$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "In the present discussion we assume that our matrix is real and symmetric, that is \n",
    "$\\mathbf{A}\\in {\\mathbb{R}}^{n\\times n}$.\n",
    "The matrix $\\mathbf{A}$ has $n$ eigenvalues\n",
    "$\\lambda_1\\dots \\lambda_n$ (distinct or not). Let $\\mathbf{D}$ be the\n",
    "diagonal matrix with the eigenvalues on the diagonal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{D}=    \\left( \\begin{array}{ccccccc} \\lambda_1 & 0 & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                0 & \\lambda_2 & 0 & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & 0 & \\lambda_3 & 0  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &\\lambda_{n-1} & \\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &0 & \\lambda_n\n",
    "             \\end{array} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathbf{A}$ is real and symmetric then there exists a real orthogonal matrix $\\mathbf{S}$ such that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{S}^T \\mathbf{A}\\mathbf{S}= \\mathrm{diag}(\\lambda_1,\\lambda_2,\\dots ,\\lambda_n),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and for $j=1:n$ we have $\\mathbf{A}\\mathbf{S}(:,j) = \\lambda_j \\mathbf{S}(:,j)$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "To obtain the eigenvalues of $\\mathbf{A}\\in {\\mathbb{R}}^{n\\times n}$,\n",
    "the strategy is to\n",
    "perform a series of similarity transformations on the original\n",
    "matrix $\\mathbf{A}$, in order to reduce it either into a  diagonal form as above\n",
    "or into a  tridiagonal form. \n",
    "\n",
    "We say that a matrix $\\mathbf{B}$ is a similarity\n",
    "transform  of  $\\mathbf{A}$ if"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{B}= \\mathbf{S}^T \\mathbf{A}\\mathbf{S}, \\hspace{1cm} \\mathrm{where} \\hspace{1cm}  \\mathbf{S}^T\\mathbf{S}=\\mathbf{S}^{-1}\\mathbf{S} =\\mathbf{I}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The importance of a similarity transformation lies in the fact that\n",
    "the resulting matrix has the same\n",
    "eigenvalues, but the eigenvectors are in general different.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "To prove this we\n",
    "start with  the eigenvalue problem and a similarity transformed matrix $\\mathbf{B}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x} \\hspace{1cm} \\mathrm{and}\\hspace{1cm} \n",
    "    \\mathbf{B}= \\mathbf{S}^T \\mathbf{A}\\mathbf{S}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We multiply the first equation on the left by $\\mathbf{S}^T$ and insert\n",
    "$\\mathbf{S}^{T}\\mathbf{S} = \\mathbf{I}$ between $\\mathbf{A}$ and $\\mathbf{x}$. Then we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto1\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   (\\mathbf{S}^T\\mathbf{A}\\mathbf{S})(\\mathbf{S}^T\\mathbf{x})=\\lambda\\mathbf{S}^T\\mathbf{x} ,\n",
    "\\label{_auto1} \\tag{1}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is the same as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{B} \\left ( \\mathbf{S}^T\\mathbf{x} \\right ) = \\lambda \\left (\\mathbf{S}^T\\mathbf{x}\\right ).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable  $\\lambda$ is an eigenvalue of $\\mathbf{B}$ as well, but with\n",
    "eigenvector $\\mathbf{S}^T\\mathbf{x}$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Eigenvalue problems, basic definitions\n",
    "The basic philosophy is to\n",
    " * Either apply subsequent similarity transformations (direct method) so that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"_auto2\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "   \\mathbf{S}_N^T\\dots \\mathbf{S}_1^T\\mathbf{A}\\mathbf{S}_1\\dots \\mathbf{S}_N=\\mathbf{D} ,\n",
    "\\label{_auto2} \\tag{2}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Or apply subsequent similarity transformations so that $\\mathbf{A}$ becomes tridiagonal (Householder) or upper/lower triangular (the *QR* method to be discussed later). \n",
    "\n",
    " * Thereafter, techniques for obtaining eigenvalues from tridiagonal matrices can be used.\n",
    "\n",
    " * Or use so-called power methods\n",
    "\n",
    " * Or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Discussion of methods for eigenvalues\n",
    "**The general overview.**\n",
    "\n",
    "\n",
    "One speaks normally of two main approaches to solving the eigenvalue problem.\n",
    " * The first is the formal method, involving determinants and the  characteristic polynomial. This proves how many eigenvalues  there are, and is the way most of you learned about how to solve the eigenvalue problem, but for matrices of dimensions greater than 2 or 3, it is rather impractical.\n",
    "\n",
    " * The other general approach is to use similarity or unitary tranformations  to reduce a matrix to diagonal form. This is normally done in two steps: first reduce to for example a *tridiagonal* form, and then to diagonal form. The main algorithms we will discuss in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this methodology.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Direct methods\n",
    "Direct or non-iterative methods  require for matrices of dimensionality $n\\times n$ typically $O(n^3)$ operations. These methods are normally called standard methods and are used for dimensionalities\n",
    "$n \\sim 10^5$ or smaller. A brief historical overview  \n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">  Year </th> <th align=\"center\">    $n$     </th> <th align=\"center\">                 </th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"center\">   1950       </td> <td align=\"center\">   $n=20$          </td> <td align=\"center\">   (Wilkinson)          </td> </tr>\n",
    "<tr><td align=\"center\">   1965       </td> <td align=\"center\">   $n=200$         </td> <td align=\"center\">   (Forsythe et al.)    </td> </tr>\n",
    "<tr><td align=\"center\">   1980       </td> <td align=\"center\">   $n=2000$        </td> <td align=\"center\">   Linpack              </td> </tr>\n",
    "<tr><td align=\"center\">   1995       </td> <td align=\"center\">   $n=20000$       </td> <td align=\"center\">   Lapack               </td> </tr>\n",
    "<tr><td align=\"center\">   Present    </td> <td align=\"center\">   $n\\sim 10^6$    </td> <td align=\"center\">   Lapack               </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of\n",
    "$10^4$. However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost $10^{15}$. We see clearly played out in history the $O(n^3)$ bottleneck  of direct matrix algorithms.\n",
    "\n",
    "Sloppily speaking, when  $n\\sim 10^4$ is cubed we have $O(10^{12})$ operations, which is smaller than the $10^{15}$ increase in flops.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Beyond Direct methods\n",
    "If the matrix to diagonalize is large and sparse, direct methods simply become impractical, \n",
    "also because\n",
    "many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the \n",
    "$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. \n",
    "Given a matrix $\\mathbf{A}$ and a vector $\\mathbf{v}$, the associated Krylov sequences of vectors\n",
    "(and thereby subspaces) \n",
    "$\\mathbf{v}$, $\\mathbf{A}\\mathbf{v}$, $\\mathbf{A}^2\\mathbf{v}$, $\\mathbf{A}^3\\mathbf{v},\\dots$, represent\n",
    "successively larger Krylov subspaces. \n",
    "\n",
    "<table border=\"1\">\n",
    "<thead>\n",
    "<tr><th align=\"center\">           Matrix           </th> <th align=\"center\">$\\mathbf{A}\\mathbf{x}=\\mathbf{b}$</th> <th align=\"center\">$\\mathbf{A}\\mathbf{x}=\\lambda\\mathbf{x}$</th> </tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr><td align=\"left\">   $\\mathbf{A}=\\mathbf{A}^*$       </td> <td align=\"left\">   Conjugate gradient                   </td> <td align=\"left\">   Lanczos                                     </td> </tr>\n",
    "<tr><td align=\"left\">   $\\mathbf{A}\\ne \\mathbf{A}^*$    </td> <td align=\"left\">   GMRES etc                            </td> <td align=\"left\">   Arnoldi                                     </td> </tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Methods for finding Eigenpairs (eigenvalues and eigenvectors)\n",
    "\n",
    "The original source codes are taken from the widely used software\n",
    "package LAPACK, which follows two other popular packages developed in the 1970s, \n",
    "namely EISPACK and LINPACK.\n",
    " * LINPACK: package for linear equations  and least square problems.\n",
    "\n",
    " * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website <http://www.netlib.org>  it is  possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.\n",
    "\n",
    " * BLAS (I, II and III): (Basic Linear Algebra Subprograms)  are routines that provide standard building blocks for performing basic vector and matrix operations.   Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The buckling beam problem, a classical wave function problem in one dimension\n",
    "\n",
    "We start with the following differential equation, namely"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\gamma \\frac{d^2 u(x)}{dx^2} = -F u(x),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $u(x)$ is the vertical displacement of the beam in the $y$ direction. \n",
    "The beam has length $L$, $x\\in [0,L]$ and $F$ is a force applied at $(L,0)$ in the direction towards the origin. \n",
    "The parameter $\\gamma$ is a constant defined by properties like the rigidity of the beam. \n",
    "We apply again so-called Dirichlet boundary conditions and set $u(0)=u(L)=0$.\n",
    "\n",
    "In this specific case two of the  parameters $\\gamma$, $F$ and $L$ are known. As an example, assume we know $F$ and $L$. Then the eigenvalue problem we set up below will allow us to find $\\gamma$. \n",
    "\n",
    "We define a dimensional variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho = \\frac{x}{L},\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "meaning that we have $\\rho \\in [0,1]$.    By reordering the equation as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{d^2 u(\\rho)}{d\\rho^2} = -\\frac{FL^2}{\\gamma} u(\\rho)=-\\lambda u(\\rho),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $\\lambda= FL^2/\\gamma$ we have an equation that when discretized, becomes an eigenvalue problem.\n",
    "We use one of the more famous \n",
    "expression for the second derivative of a function $u$ (called a [three-point](https://github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Lectures/lectures2015.pdf) formula to be derived  on whiteboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:diffoperation\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    u''=\\frac{u(\\rho+h) -2u(\\rho) +u(\\rho-h)}{h^2} +O(h^2),\n",
    "\\label{eq:diffoperation} \\tag{3}\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $h$ is a so-called  step size.\n",
    "\n",
    "## Min and Max values for the variable $\\rho$\n",
    "\n",
    "Next we define minimum and maximum values for the variable $\\rho$,\n",
    "$\\rho_{\\mathrm{min}}=0$  and $\\rho_{\\mathrm{max}}=1$, respectively.\n",
    "With a given number of mesh points, $n$, we \n",
    "define the step length $h$ as, with $\\rho_{\\mathrm{min}}=\\rho_0$  and $\\rho_{\\mathrm{max}}=\\rho_n$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "h=\\frac{\\rho_N-\\rho_0 }{n}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of $\\rho$ at a point $i$ is then"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\rho_i= \\rho_0 + ih \\hspace{1cm} i=1,2,\\dots , n.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rewrite the differential equation for a value $\\rho_i$ as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{u(\\rho_i+h) -2u(\\rho_i) +u(\\rho_i-h)}{h^2}  = \\lambda u(\\rho_i),\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in  a more compact way as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "-\\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}  = \\lambda u_i.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewriting the equation as an eigenvalue problem\n",
    "\n",
    "We can rewrite this equation in a more a general form, but now \n",
    "as an eigenvalue problem, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Equation labels as ordinary links -->\n",
    "<div id=\"eq:matrixse\"></div>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\begin{bmatrix} d& a & 0   & 0    & \\dots  &0     & 0 \\\\\n",
    "                                a & d & a & 0    & \\dots  &0     &0 \\\\\n",
    "                                0   & a & d & a  &0       &\\dots & 0\\\\\n",
    "                                \\dots  & \\dots & \\dots & \\dots  &\\dots      &\\dots & \\dots\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &a  &d & a\\\\\n",
    "                                0   & \\dots & \\dots & \\dots  &\\dots       &a & d\\end{bmatrix} \n",
    "                                 \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\dots \\\\ u_{n-2} \\\\ u_{n-1}\\end{bmatrix} = \\lambda \\begin{bmatrix} u_1 \\\\ u_2 \\\\ u_3 \\\\ \\dots \\\\ u_{n-2} \\\\ u_{n-1}\\end{bmatrix} . \n",
    "\\label{eq:matrixse} \\tag{4} \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have not included the endpoints $u_0$ and $u_n$. \n",
    "We have defined $d=2/h^2$ and the non-diagonal ones as $a=-1/h^2$. This eigenvalue problem has analytical eigenpairs, with eigenvalues given as (note that we define the matrix to run from $i=1$ to $i=N-1$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\lambda_j = d+2a\\cos{(\\frac{j\\pi}{n})}, \\hspace{0.2cm} j=1,2,\\dots n-1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eigenvectors are"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{u}_j = [ \\sin(\\frac{j\\pi}{n}), \\sin(\\frac{2j\\pi}{n}), ..., \\sin(\\frac{(N-1)j\\pi}{n}) ]^T, \\hspace{0.2cm} j=1,2,\\dots n-1.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Diagonalizing a Tridiagonal Toeplitz matrix\n",
    "from math import cos, pi, log10\n",
    "import numpy as np\n",
    "#Function for initialization of parameters\n",
    "def initialize():\n",
    "    RMin = 0.0\n",
    "    RMax = 1.0\n",
    "    n = 10\n",
    "    return RMin, RMax, n\n",
    "\n",
    "#Get the boundary and number of integration points\n",
    "RMin, RMax, n = initialize()\n",
    "\n",
    "#Initialize constants, step size and the constant values of the diagonal and non-diagonal elements\n",
    "Step    = RMax/(n+1)\n",
    "DiagConst = 2.0 / (Step*Step)\n",
    "NondiagConst =  -1.0 / (Step*Step)\n",
    "\n",
    "\n",
    "#Setting up a tridiagonal matrix\n",
    "Hamiltonian = np.zeros((n,n))\n",
    "Hamiltonian[0,0] = DiagConst\n",
    "Hamiltonian[0,1] = NondiagConst\n",
    "for i in range(1,n-1):\n",
    "    Hamiltonian[i,i-1]  = NondiagConst\n",
    "    Hamiltonian[i,i]    = DiagConst\n",
    "    Hamiltonian[i,i+1]  = NondiagConst\n",
    "Hamiltonian[n-1,n-2] = NondiagConst\n",
    "Hamiltonian[n-1,n-1] = DiagConst\n",
    "# diagonalize and obtain eigenvalues, not necessarily sorted\n",
    "EigValues, EigVectors = np.linalg.eig(Hamiltonian)\n",
    "# sort eigenvectors and eigenvalues\n",
    "permute = EigValues.argsort()\n",
    "EigValues = EigValues[permute]\n",
    "EigVectors = EigVectors[:,permute]\n",
    "# compute  the difference between the numerical and exact eigenvalues\n",
    "for i in range(n):\n",
    "    lambda_i = DiagConst+2*NondiagConst*cos((i+1)*pi*Step)\n",
    "    print(abs(EigValues[i]-lambda_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 1: Mathematical intermezzo\n",
    "\n",
    "A unitary transformation preserves  the orthogonality of the obtained eigenvectors. To see this consider first a basis of vectors $\\mathbf{v}_i$,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_i = \\begin{bmatrix} v_{i1} \\\\ \\dots \\\\ \\dots \\\\v_{in} \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that the basis is orthogonal, that is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{v}_j^T\\mathbf{v}_i = \\delta_{ij}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show that an orthogonal or unitary transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{w}_i=\\mathbf{U}\\mathbf{v}_i,\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preserves the dot product and orthogonality.\n",
    "\n",
    "<!-- --- end exercise --- -->\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- begin exercise --- -->\n",
    "\n",
    "## Exercise 2: Setting up a code for tridiagonal Toeplitz matrix\n",
    "\n",
    "\n",
    "**a)**\n",
    "Convince your self about the proper scaling of the differential equation in terms of a dimensionless length parameter. What is the dimension of $u(x)$? What is the dimension of the parameter $\\gamma$ in the differential equation for the buckling beam/spring?\n",
    "\n",
    "**b)**\n",
    "Convince yourself that the differential equation, when discretized, can be rewritten as an eigenvalue problem.\n",
    "\n",
    "**c)**\n",
    "Use the included python code here or write your own and compare the analytical eigenvalues with the numerical ones for $n=4$, $n=10$ and $n=100$. Comment your results.\n",
    "\n",
    "**d)**\n",
    "Plot the eigenvectors for the three lowest eigenvalues and comment your results.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<!-- --- end exercise --- -->"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
