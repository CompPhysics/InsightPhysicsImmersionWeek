TITLE: INSIGHT Physics Immersion Week (June 21 - June 25), Eigenvalue problems, Lecture Wednesday June 23
AUTHOR: Morten Hjorth-Jensen {copyright, 1999-present|CC BY-NC} at Department of Physics, University of Oslo & Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University
DATE: June 23, 2021



!split
===== Eigenvalue problems, basic definitions =====
!bblock 
Let us consider the matrix $\mathbf{A}$ of dimension $n$. The eigenvalues of
$\mathbf{A}$ are defined through the matrix equation 
!bt
\[
   \mathbf{A}\mathbf{x}^{(\nu)} = \lambda^{(\nu)}\mathbf{x}^{(\nu)},
\]
!et
where $\lambda^{(\nu)}$ are the eigenvalues and $\mathbf{x}^{(\nu)}$ the
corresponding eigenvectors.
Unless otherwise stated, when we use the wording eigenvector we mean the
right eigenvector. The left eigenvalue problem is defined as 
!bt
\[
\mathbf{x}^{(\nu)}_L\mathbf{A} = \lambda^{(\nu)}\mathbf{x}^{(\nu)}_L
\]
!et
The above right eigenvector problem is equivalent to a set of $n$ equations with $n$ unknowns
$x_i$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
The eigenvalue problem can be rewritten as 
!bt
\[
   \left( \mathbf{A}-\lambda^{(\nu)} \mathbf{I} \right) \mathbf{x}^{(\nu)} = 0,
\]
!et
with $\mathbf{I}$ being the unity matrix. This equation provides
a solution to the problem if and only if the determinant
is zero, namely
!bt
\[
   \left| \mathbf{A}-\lambda^{(\nu)}\mathbf{I}\right| = 0,
\]
!et
which in turn means that the determinant is a polynomial
of degree $n$ in $\lambda$ and in general we will have 
$n$ distinct zeros. 
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
The eigenvalues of a matrix 
$\mathbf{A}\in {\mathbb{C}}^{n\times n}$
are thus the $n$ roots of its characteristic polynomial 
!bt
\[
P(\lambda) = det(\lambda\mathbf{I}-\mathbf{A}),
\]
!et
or 
!bt
\[
  P(\lambda)= \prod_{i=1}^{n}\left(\lambda_i-\lambda\right).
\]
!et
The set of these roots is called the spectrum and is denoted as
$\lambda(\mathbf{A})$.
If $\lambda(\mathbf{A})=\left\{\lambda_1,\lambda_2,\dots ,\lambda_n\right\}$ then we have
!bt
\[
   det(\mathbf{A})= \lambda_1\lambda_2\dots\lambda_n, 
\]
!et
and if we define the trace of $\mathbf{A}$ as
!bt
\[
Tr(\mathbf{A})=\sum_{i=1}^n a_{ii}\]
!et
then
!bt
\[
Tr(\mathbf{A})=\lambda_1+\lambda_2+\dots+\lambda_n.
\]
!et
!eblock


!split
===== Abel-Ruffini Impossibility Theorem =====
!bblock 
The *Abel-Ruffini* theorem (also known as Abel's impossibility theorem) 
states that there is no general solution in radicals to polynomial equations of degree five or higher.

The content of this theorem is frequently misunderstood. It does not assert that higher-degree polynomial equations are unsolvable. 
In fact, if the polynomial has real or complex coefficients, and we allow complex solutions, then every polynomial equation has solutions; this is the fundamental theorem of algebra. Although these solutions cannot always be computed exactly with radicals, they can be computed to any desired degree of accuracy using numerical methods such as the Newton-Raphson method or Laguerre method, and in this way they are no different from solutions to polynomial equations of the second, third, or fourth degrees.

The theorem only concerns the form that such a solution must take. The content of the theorem is 
that the solution of a higher-degree equation cannot in all cases be expressed in terms of the polynomial coefficients with a finite number of operations of addition, subtraction, multiplication, division and root extraction. Some polynomials of arbitrary degree, of which the simplest nontrivial example is the monomial equation $ax^n = b$, are always solvable with a radical.
!eblock


!split
===== Abel-Ruffini Impossibility Theorem =====
!bblock 

The *Abel-Ruffini* theorem says that there are some fifth-degree equations whose solution cannot be so expressed. 
The equation $x^5 - x + 1 = 0$ is an example. Some other fifth degree equations can be solved by radicals, 
for example $x^5 - x^4 - x + 1 = 0$. The precise criterion that distinguishes between those equations that can be solved 
by radicals and those that cannot was given by Galois and is now part of Galois theory: 
a polynomial equation can be solved by radicals if and only if its Galois group is a solvable group.

Today, in the modern algebraic context, we say that second, third and fourth degree polynomial 
equations can always be solved by radicals because the symmetric groups $S_2, S_3$ and $S_4$ are solvable groups, 
whereas $S_n$ is not solvable for $n \ge 5$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
In the present discussion we assume that our matrix is real and symmetric, that is 
$\mathbf{A}\in {\mathbb{R}}^{n\times n}$.
The matrix $\mathbf{A}$ has $n$ eigenvalues
$\lambda_1\dots \lambda_n$ (distinct or not). Let $\mathbf{D}$ be the
diagonal matrix with the eigenvalues on the diagonal
!bt
\[
\mathbf{D}=    \left( \begin{array}{ccccccc} \lambda_1 & 0 & 0   & 0    & \dots  &0     & 0 \\
                                0 & \lambda_2 & 0 & 0    & \dots  &0     &0 \\
                                0   & 0 & \lambda_3 & 0  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &\dots       &\lambda_{n-1} & \\
                                0   & \dots & \dots & \dots  &\dots       &0 & \lambda_n

             \end{array} \right).
\]
!et
If $\mathbf{A}$ is real and symmetric then there exists a real orthogonal matrix $\mathbf{S}$ such that
!bt
\[
     \mathbf{S}^T \mathbf{A}\mathbf{S}= \mathrm{diag}(\lambda_1,\lambda_2,\dots ,\lambda_n),
\]
!et
and for $j=1:n$ we have $\mathbf{A}\mathbf{S}(:,j) = \lambda_j \mathbf{S}(:,j)$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
To obtain the eigenvalues of $\mathbf{A}\in {\mathbb{R}}^{n\times n}$,
the strategy is to
perform a series of similarity transformations on the original
matrix $\mathbf{A}$, in order to reduce it either into a  diagonal form as above
or into a  tridiagonal form. 

We say that a matrix $\mathbf{B}$ is a similarity
transform  of  $\mathbf{A}$ if 
!bt
\[
     \mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S}, \hspace{1cm} \mathrm{where} \hspace{1cm}  \mathbf{S}^T\mathbf{S}=\mathbf{S}^{-1}\mathbf{S} =\mathbf{I}.
\]
!et
The importance of a similarity transformation lies in the fact that
the resulting matrix has the same
eigenvalues, but the eigenvectors are in general different. 
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock 
To prove this we
start with  the eigenvalue problem and a similarity transformed matrix $\mathbf{B}$.
!bt
\[
   \mathbf{A}\mathbf{x}=\lambda\mathbf{x} \hspace{1cm} \mathrm{and}\hspace{1cm} 
    \mathbf{B}= \mathbf{S}^T \mathbf{A}\mathbf{S}.
\]
!et
We multiply the first equation on the left by $\mathbf{S}^T$ and insert
$\mathbf{S}^{T}\mathbf{S} = \mathbf{I}$ between $\mathbf{A}$ and $\mathbf{x}$. Then we get
!bt
\begin{equation}
   (\mathbf{S}^T\mathbf{A}\mathbf{S})(\mathbf{S}^T\mathbf{x})=\lambda\mathbf{S}^T\mathbf{x} ,
\end{equation}  
!et
which is the same as 
!bt
\[
   \mathbf{B} \left ( \mathbf{S}^T\mathbf{x} \right ) = \lambda \left (\mathbf{S}^T\mathbf{x}\right ).
\]
!et
The variable  $\lambda$ is an eigenvalue of $\mathbf{B}$ as well, but with
eigenvector $\mathbf{S}^T\mathbf{x}$.
!eblock


!split
===== Eigenvalue problems, basic definitions =====
!bblock  
The basic philosophy is to
 * Either apply subsequent similarity transformations (direct method) so that 
!bt
\begin{equation}
   \mathbf{S}_N^T\dots \mathbf{S}_1^T\mathbf{A}\mathbf{S}_1\dots \mathbf{S}_N=\mathbf{D} ,
\end{equation}
!et
 * Or apply subsequent similarity transformations so that $\mathbf{A}$ becomes tridiagonal (Householder) or upper/lower triangular (the *QR* method to be discussed later). 
 * Thereafter, techniques for obtaining eigenvalues from tridiagonal matrices can be used.
 * Or use so-called power methods
 * Or use iterative methods (Krylov, Lanczos, Arnoldi). These methods are popular for huge matrix problems.
!eblock



!split
===== Discussion of methods for eigenvalues =====
!bblock The general overview

One speaks normally of two main approaches to solving the eigenvalue problem.
 * The first is the formal method, involving determinants and the  characteristic polynomial. This proves how many eigenvalues  there are, and is the way most of you learned about how to solve the eigenvalue problem, but for matrices of dimensions greater than 2 or 3, it is rather impractical.

 * The other general approach is to use similarity or unitary tranformations  to reduce a matrix to diagonal form. This is normally done in two steps: first reduce to for example a *tridiagonal* form, and then to diagonal form. The main algorithms we will discuss in detail, Jacobi's and  Householder's  (so-called direct method) and Lanczos algorithms (an iterative method), follow this methodology. 
!eblock


!split
===== Direct methods  =====
!bblock 
Direct or non-iterative methods  require for matrices of dimensionality $n\times n$ typically $O(n^3)$ operations. These methods are normally called standard methods and are used for dimensionalities
$n \sim 10^5$ or smaller. A brief historical overview  

|----------------------------------------|
| Year  | $n$          |                 |
|----------------------------------------|
|  1950 | $n=20$       |(Wilkinson)      |
|  1965 | $n=200$      |(Forsythe et al.)|
|  1980 | $n=2000$     |Linpack          |
|  1995 | $n=20000$    |Lapack           |
|  Present | $n\sim 10^6$ |Lapack           |
|----------------------------------------|

shows that in the course of 60 years the dimension that  direct diagonalization methods can handle  has increased by almost a factor of
$10^4$. However, it pales beside the progress achieved by computer hardware, from flops to petaflops, a factor of almost $10^{15}$. We see clearly played out in history the $O(n^3)$ bottleneck  of direct matrix algorithms.

Sloppily speaking, when  $n\sim 10^4$ is cubed we have $O(10^{12})$ operations, which is smaller than the $10^{15}$ increase in flops.  

!eblock


!split
===== Beyond Direct methods  =====
!bblock 
If the matrix to diagonalize is large and sparse, direct methods simply become impractical, 
also because
many of the direct methods tend to destroy sparsity. As a result large dense matrices may arise during the diagonalization procedure.  The idea behind iterative methods is to project the 
$n-$dimensional problem in smaller spaces, so-called Krylov subspaces. 
Given a matrix $\mathbf{A}$ and a vector $\mathbf{v}$, the associated Krylov sequences of vectors
(and thereby subspaces) 
$\mathbf{v}$, $\mathbf{A}\mathbf{v}$, $\mathbf{A}^2\mathbf{v}$, $\mathbf{A}^3\mathbf{v},\dots$, represent
successively larger Krylov subspaces. 

|----------------------------------------------------------------------|
| Matrix |$\mathbf{A}\mathbf{x}=\mathbf{b}$ |$\mathbf{A}\mathbf{x}=\lambda\mathbf{x}$    | 
|--------------l----------------l---------------------l----------------|
|   $\mathbf{A}=\mathbf{A}^*$ | Conjugate gradient    | Lanczos              |
|  $\mathbf{A}\ne \mathbf{A}^*$  |GMRES etc           |Arnoldi               |
|----------------------------------------------------------------------|

!eblock


!split
===== Methods for finding Eigenpairs (eigenvalues and eigenvectors) =====
!bblock 

The original source codes are taken from the widely used software
package LAPACK, which follows two other popular packages developed in the 1970s, 
namely EISPACK and LINPACK.
 * LINPACK: package for linear equations  and least square problems.
 * LAPACK:package for solving symmetric, unsymmetric and generalized eigenvalue problems. From LAPACK's website URL: "http://www.netlib.org"  it is  possible to download for free all source codes from this library. Both C/C++ and Fortran versions are available.
 * BLAS (I, II and III): (Basic Linear Algebra Subprograms)  are routines that provide standard building blocks for performing basic vector and matrix operations.   Blas I is vector operations, II vector-matrix operations and III matrix-matrix operations. 

!eblock



!split
===== The buckling beam problem, a classical wave function problem in one dimension =====

We start with the following differential equation, namely 
!bt
\[
\gamma \frac{d^2 u(x)}{dx^2} = -F u(x),
\]
!et
where $u(x)$ is the vertical displacement of the beam in the $y$ direction. 
The beam has length $L$, $x\in [0,L]$ and $F$ is a force applied at $(L,0)$ in the direction towards the origin. 
The parameter $\gamma$ is a constant defined by properties like the rigidity of the beam. 
We apply again so-called Dirichlet boundary conditions and set $u(0)=u(L)=0$.

In this specific case two of the  parameters $\gamma$, $F$ and $L$ are known. As an example, assume we know $F$ and $L$. Then the eigenvalue problem we set up below will allow us to find $\gamma$. 

We define a dimensional variable
!bt
\[
 \rho = \frac{x}{L}, 
\]
!et
meaning that we have $\rho \in [0,1]$.    By reordering the equation as 
!bt
\[
 \frac{d^2 u(\rho)}{d\rho^2} = -\frac{FL^2}{\gamma} u(\rho)=-\lambda u(\rho),
\]
!et
with $\lambda= FL^2/\gamma$ we have an equation that when discretized, becomes an eigenvalue problem.
We use one of the more famous 
expression for the second derivative of a function $u$ (called a "three-point":"https://github.com/CompPhysics/ComputationalPhysics/blob/master/doc/Lectures/lectures2015.pdf" formula to be derived  on whiteboard)
!bt
\begin{equation}
    u''=\frac{u(\rho+h) -2u(\rho) +u(\rho-h)}{h^2} +O(h^2),
    label{eq:diffoperation}
\end{equation}
!et 
where $h$ is a so-called  step size.

!split
===== Min and Max values for the variable $\rho$ =====

Next we define minimum and maximum values for the variable $\rho$,
$\rho_{\mathrm{min}}=0$  and $\rho_{\mathrm{max}}=1$, respectively.
With a given number of mesh points, $n$, we 
define the step length $h$ as, with $\rho_{\mathrm{min}}=\rho_0$  and $\rho_{\mathrm{max}}=\rho_n$,
!bt
\begin{equation*}
  h=\frac{\rho_N-\rho_0 }{n}.
\end{equation*}
!et
The value of $\rho$ at a point $i$ is then 
!bt
\[
    \rho_i= \rho_0 + ih \hspace{1cm} i=1,2,\dots , n.
\]
!et
We can rewrite the differential equation for a value $\rho_i$ as
!bt
\[
-\frac{u(\rho_i+h) -2u(\rho_i) +u(\rho_i-h)}{h^2}  = \lambda u(\rho_i),
\]
!et
or in  a more compact way as
!bt
\[
-\frac{u_{i+1} -2u_i +u_{i-1} }{h^2}  = \lambda u_i.
\]
!et

!split
=====  Rewriting the equation as an eigenvalue problem =====

We can rewrite this equation in a more a general form, but now 
as an eigenvalue problem, that is
!bt
\begin{equation}
    \begin{bmatrix} d& a & 0   & 0    & \dots  &0     & 0 \\
                                a & d & a & 0    & \dots  &0     &0 \\
                                0   & a & d & a  &0       &\dots & 0\\
                                \dots  & \dots & \dots & \dots  &\dots      &\dots & \dots\\
                                0   & \dots & \dots & \dots  &a  &d & a\\
                                0   & \dots & \dots & \dots  &\dots       &a & d\end{bmatrix} 
                                 \begin{bmatrix} u_1 \\ u_2 \\ u_3 \\ \dots \\ u_{n-2} \\ u_{n-1}\end{bmatrix} = \lambda \begin{bmatrix} u_1 \\ u_2 \\ u_3 \\ \dots \\ u_{n-2} \\ u_{n-1}\end{bmatrix} . 
label{eq:matrixse} 
\end{equation}
!et 

We have not included the endpoints $u_0$ and $u_n$. 
We have defined $d=2/h^2$ and the non-diagonal ones as $a=-1/h^2$. This eigenvalue problem has analytical eigenpairs, with eigenvalues given as (note that we define the matrix to run from $i=1$ to $i=N-1$)
!bt
\[
\lambda_j = d+2a\cos{(\frac{j\pi}{n})}, \hspace{0.2cm} j=1,2,\dots n-1.
\]
!et

The eigenvectors are 
!bt
\[
\mathbf{u}_j = [ \sin(\frac{j\pi}{n}), \sin(\frac{2j\pi}{n}), ..., \sin(\frac{(N-1)j\pi}{n}) ]^T, \hspace{0.2cm} j=1,2,\dots n-1.
\]
!et

!split
===== Code example =====
!bc pycod
# Diagonalizing a Tridiagonal Toeplitz matrix
from math import cos, pi, log10
import numpy as np
#Function for initialization of parameters
def initialize():
    RMin = 0.0
    RMax = 1.0
    n = 10
    return RMin, RMax, n

#Get the boundary and number of integration points
RMin, RMax, n = initialize()

#Initialize constants, step size and the constant values of the diagonal and non-diagonal elements
Step    = RMax/(n+1)
DiagConst = 2.0 / (Step*Step)
NondiagConst =  -1.0 / (Step*Step)


#Setting up a tridiagonal matrix
Hamiltonian = np.zeros((n,n))
Hamiltonian[0,0] = DiagConst
Hamiltonian[0,1] = NondiagConst
for i in range(1,n-1):
    Hamiltonian[i,i-1]  = NondiagConst
    Hamiltonian[i,i]    = DiagConst
    Hamiltonian[i,i+1]  = NondiagConst
Hamiltonian[n-1,n-2] = NondiagConst
Hamiltonian[n-1,n-1] = DiagConst
# diagonalize and obtain eigenvalues, not necessarily sorted
EigValues, EigVectors = np.linalg.eig(Hamiltonian)
# sort eigenvectors and eigenvalues
permute = EigValues.argsort()
EigValues = EigValues[permute]
EigVectors = EigVectors[:,permute]
# compute  the difference between the numerical and exact eigenvalues
for i in range(n):
    lambda_i = DiagConst+2*NondiagConst*cos((i+1)*pi*Step)
    print(abs(EigValues[i]-lambda_i))

!ec


===== Exercise: Mathematical intermezzo =====

A unitary transformation preserves  the orthogonality of the obtained eigenvectors. To see this consider first a basis of vectors $\mathbf{v}_i$,
!bt
\[
\mathbf{v}_i = \begin{bmatrix} v_{i1} \\ \dots \\ \dots \\v_{in} \end{bmatrix}
\]
!et
We assume that the basis is orthogonal, that is 
!bt
\[
\mathbf{v}_j^T\mathbf{v}_i = \delta_{ij}.
\]
!et

Show that an orthogonal or unitary transformation
!bt
\[
\mathbf{w}_i=\mathbf{U}\mathbf{v}_i,
\]
!et
preserves the dot product and orthogonality. 


===== Exercise: Setting up a code for tridiagonal Toeplitz matrix =====

!bsubex
Convince your self about the proper scaling of the differential equation in terms of a dimensionless length parameter. What is the dimension of $u(x)$? What is the dimension of the parameter $\gamma$ in the differential equation for the buckling beam/spring?
!esubex

!bsubex
Convince yourself that the differential equation, when discretized, can be rewritten as an eigenvalue problem.
!esubex

!bsubex
Use the included python code here or write your own and compare the analytical eigenvalues with the numerical ones for $n=4$, $n=10$ and $n=100$. Comment your results.
!esubex

!bsubex
Plot the eigenvectors for the three lowest eigenvalues and comment your results.
!esubex







